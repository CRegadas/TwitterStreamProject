âžœ  spark-1.4.0-bin-hadoop2.6  ./bin/spark-submit --class "Main.Main" --master spark://macbookarura.lan:7077 /Users/sindz/MEI/Dissertacao/TwitterStreamProject/target/scala-2.10/hello-assembly-1.0.jar



log4j:WARN No appenders could be found for logger (kafka.utils.VerifiableProperties).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
------------------------------------------MAIN_service kafka
STATUS: RT @PostGradProblem: In preparation for the NFL lockout, I will be spending twice as much time analyzing my fantasy baseball team during ...
KafkaService_WRITE_STATUS
--------------------------------------A GUARDAR USER NO REDIS
STATUS: TETSTETETEETET RT @PostGradProblem: In preparation for the NFL lockout, I will be spending twice as much time analyzing my fantasy baseball team during ...
KafkaService_WRITE_STATUS
--------------------------------------A GUARDAR USER NO REDIS
------------------------------------------MAIN_filter control
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/06/30 23:11:01 INFO SparkContext: Running Spark version 1.4.0
2015-06-30 23:11:01.807 java[71026:2780522] Unable to load realm info from SCDynamicStore
15/06/30 23:11:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/06/30 23:11:02 INFO SecurityManager: Changing view acls to: sindz
15/06/30 23:11:02 INFO SecurityManager: Changing modify acls to: sindz
15/06/30 23:11:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sindz); users with modify permissions: Set(sindz)
15/06/30 23:11:02 INFO Slf4jLogger: Slf4jLogger started
15/06/30 23:11:02 INFO Remoting: Starting remoting
15/06/30 23:11:02 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.101:61003]
15/06/30 23:11:02 INFO Utils: Successfully started service 'sparkDriver' on port 61003.
15/06/30 23:11:02 INFO SparkEnv: Registering MapOutputTracker
15/06/30 23:11:02 INFO SparkEnv: Registering BlockManagerMaster
15/06/30 23:11:02 INFO DiskBlockManager: Created local directory at /private/var/folders/cq/rjz7hsgn2w3bjqvvpxglsvfh0000gn/T/spark-72b09a51-f959-4312-a811-59cf88e50f6b/blockmgr-fa5271d6-2b34-4072-8835-d39cf039d0dc
15/06/30 23:11:02 INFO MemoryStore: MemoryStore started with capacity 265.4 MB
15/06/30 23:11:03 INFO HttpFileServer: HTTP File server directory is /private/var/folders/cq/rjz7hsgn2w3bjqvvpxglsvfh0000gn/T/spark-72b09a51-f959-4312-a811-59cf88e50f6b/httpd-29fa62f5-03bf-4764-a3a8-16529bf3a49c
15/06/30 23:11:03 INFO HttpServer: Starting HTTP Server
15/06/30 23:11:03 INFO Utils: Successfully started service 'HTTP file server' on port 61005.
15/06/30 23:11:03 INFO SparkEnv: Registering OutputCommitCoordinator
15/06/30 23:11:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/06/30 23:11:03 INFO SparkUI: Started SparkUI at http://192.168.1.101:4040
15/06/30 23:11:03 INFO SparkContext: Added JAR file:/Users/sindz/MEI/Dissertacao/TwitterStreamProject/target/scala-2.10/hello-assembly-1.0.jar at http://192.168.1.101:61005/jars/hello-assembly-1.0.jar with timestamp 1435702263497
15/06/30 23:11:03 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@macbookarura.lan:7077/user/Master...
15/06/30 23:11:03 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150630231103-0024
15/06/30 23:11:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61008.
15/06/30 23:11:04 INFO NettyBlockTransferService: Server created on 61008
15/06/30 23:11:04 INFO BlockManagerMaster: Trying to register BlockManager
15/06/30 23:11:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.101:61008 with 265.4 MB RAM, BlockManagerId(driver, 192.168.1.101, 61008)
15/06/30 23:11:04 INFO BlockManagerMaster: Registered BlockManager
15/06/30 23:11:04 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
------------------------------------------FILTER_CONTROL
------------------------------------------SPARK_COLLECT
KAFKA_STREAM: ()
------------------------------------------TESTE: ()
HASHTAGS no FILTER_CONTROL recolhidas do Kafka: ()
15/06/30 23:11:04 INFO ReceiverTracker: ReceiverTracker started
15/06/30 23:11:04 INFO ForEachDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO KafkaInputDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO KafkaInputDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@f5ec40
15/06/30 23:11:04 INFO ForEachDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO ForEachDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO ForEachDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1caa4160
15/06/30 23:11:04 INFO ForEachDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO MappedDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO KafkaInputDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO KafkaInputDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@f5ec40
15/06/30 23:11:04 INFO MappedDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO MappedDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO MappedDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@7ca03522
15/06/30 23:11:04 INFO ForEachDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO ForEachDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO ForEachDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@60f8df02
15/06/30 23:11:04 INFO ForEachDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO FlatMappedDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO MappedDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO KafkaInputDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO KafkaInputDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@f5ec40
15/06/30 23:11:04 INFO MappedDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO MappedDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO MappedDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@7ca03522
15/06/30 23:11:04 INFO FlatMappedDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO FlatMappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO FlatMappedDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO FlatMappedDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO FlatMappedDStream: Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@379d8fc2
15/06/30 23:11:04 INFO ForEachDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO ForEachDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO ForEachDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2ed92da1
15/06/30 23:11:04 INFO ForEachDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO FlatMappedDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO MappedDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO KafkaInputDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO KafkaInputDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@f5ec40
15/06/30 23:11:04 INFO MappedDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO MappedDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO MappedDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@7ca03522
15/06/30 23:11:04 INFO FlatMappedDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO FlatMappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO FlatMappedDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO FlatMappedDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO FlatMappedDStream: Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@379d8fc2
15/06/30 23:11:04 INFO ForEachDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO ForEachDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO ForEachDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@482aa91a
15/06/30 23:11:04 INFO ForEachDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO FlatMappedDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO MappedDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: metadataCleanupDelay = -1
15/06/30 23:11:04 INFO KafkaInputDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO KafkaInputDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO KafkaInputDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO KafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@f5ec40
15/06/30 23:11:04 INFO MappedDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO MappedDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO MappedDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@7ca03522
15/06/30 23:11:04 INFO FlatMappedDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO FlatMappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO FlatMappedDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO FlatMappedDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO FlatMappedDStream: Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@379d8fc2
15/06/30 23:11:04 INFO ForEachDStream: Slide time = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/06/30 23:11:04 INFO ForEachDStream: Checkpoint interval = null
15/06/30 23:11:04 INFO ForEachDStream: Remember duration = 2000 ms
15/06/30 23:11:04 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5c241a51
15/06/30 23:11:04 INFO SparkContext: Starting job: start at Spark.scala:40
15/06/30 23:11:04 INFO RecurringTimer: Started timer for JobGenerator at time 1435702266000
15/06/30 23:11:04 INFO JobGenerator: Started JobGenerator at 1435702266000 ms
15/06/30 23:11:04 INFO JobScheduler: Started JobScheduler
15/06/30 23:11:04 INFO StreamingContext: StreamingContext started
15/06/30 23:11:04 INFO DAGScheduler: Registering RDD 2 (start at Spark.scala:40)
15/06/30 23:11:04 INFO DAGScheduler: Got job 0 (start at Spark.scala:40) with 20 output partitions (allowLocal=false)
15/06/30 23:11:04 INFO DAGScheduler: Final stage: ResultStage 1(start at Spark.scala:40)
15/06/30 23:11:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15/06/30 23:11:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15/06/30 23:11:04 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at start at Spark.scala:40), which has no missing parents
15/06/30 23:11:04 INFO MemoryStore: ensureFreeSpace(2872) called with curMem=0, maxMem=278302556
15/06/30 23:11:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.8 KB, free 265.4 MB)
15/06/30 23:11:04 INFO MemoryStore: ensureFreeSpace(1695) called with curMem=2872, maxMem=278302556
15/06/30 23:11:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1695.0 B, free 265.4 MB)
15/06/30 23:11:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.101:61008 (size: 1695.0 B, free: 265.4 MB)
15/06/30 23:11:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:874
15/06/30 23:11:04 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at start at Spark.scala:40)
15/06/30 23:11:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 50 tasks
15/06/30 23:11:06 INFO JobScheduler: Added jobs for time 1435702266000 ms
15/06/30 23:11:06 INFO JobScheduler: Starting job streaming job 1435702266000 ms.0 from job set of time 1435702266000 ms
15/06/30 23:11:06 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:06 INFO DAGScheduler: Job 1 finished: foreachRDD at Spark.scala:34, took 0.001264 s
15/06/30 23:11:06 INFO JobScheduler: Finished job streaming job 1435702266000 ms.0 from job set of time 1435702266000 ms
15/06/30 23:11:06 INFO JobScheduler: Starting job streaming job 1435702266000 ms.1 from job set of time 1435702266000 ms
-------------------------------------------
Time: 1435702266000 ms
-------------------------------------------

15/06/30 23:11:06 INFO JobScheduler: Finished job streaming job 1435702266000 ms.1 from job set of time 1435702266000 ms
15/06/30 23:11:06 INFO JobScheduler: Starting job streaming job 1435702266000 ms.2 from job set of time 1435702266000 ms
15/06/30 23:11:06 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:06 INFO DAGScheduler: Job 2 finished: foreachRDD at Spark.scala:55, took 0.000017 s
15/06/30 23:11:06 INFO JobScheduler: Finished job streaming job 1435702266000 ms.2 from job set of time 1435702266000 ms
15/06/30 23:11:06 INFO JobScheduler: Starting job streaming job 1435702266000 ms.3 from job set of time 1435702266000 ms
-------------------------------------------
Time: 1435702266000 ms
-------------------------------------------

15/06/30 23:11:06 INFO JobScheduler: Finished job streaming job 1435702266000 ms.3 from job set of time 1435702266000 ms
15/06/30 23:11:06 INFO JobScheduler: Starting job streaming job 1435702266000 ms.4 from job set of time 1435702266000 ms
15/06/30 23:11:06 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:06 INFO DAGScheduler: Job 3 finished: foreachRDD at FilterControl.scala:19, took 0.000015 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:06 INFO JobScheduler: Finished job streaming job 1435702266000 ms.4 from job set of time 1435702266000 ms
15/06/30 23:11:06 INFO JobScheduler: Total delay: 0.105 s for time 1435702266000 ms (execution: 0.059 s)
15/06/30 23:11:06 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/06/30 23:11:06 INFO InputInfoTracker: remove old batch metadata:
15/06/30 23:11:08 INFO JobScheduler: Added jobs for time 1435702268000 ms
15/06/30 23:11:08 INFO JobScheduler: Starting job streaming job 1435702268000 ms.0 from job set of time 1435702268000 ms
15/06/30 23:11:08 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:08 INFO DAGScheduler: Job 4 finished: foreachRDD at Spark.scala:34, took 0.000019 s
15/06/30 23:11:08 INFO JobScheduler: Finished job streaming job 1435702268000 ms.0 from job set of time 1435702268000 ms
15/06/30 23:11:08 INFO JobScheduler: Starting job streaming job 1435702268000 ms.1 from job set of time 1435702268000 ms
-------------------------------------------
Time: 1435702268000 ms
-------------------------------------------

15/06/30 23:11:08 INFO JobScheduler: Finished job streaming job 1435702268000 ms.1 from job set of time 1435702268000 ms
15/06/30 23:11:08 INFO JobScheduler: Starting job streaming job 1435702268000 ms.2 from job set of time 1435702268000 ms
15/06/30 23:11:08 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:08 INFO DAGScheduler: Job 5 finished: foreachRDD at Spark.scala:55, took 0.000016 s
15/06/30 23:11:08 INFO JobScheduler: Finished job streaming job 1435702268000 ms.2 from job set of time 1435702268000 ms
15/06/30 23:11:08 INFO JobScheduler: Starting job streaming job 1435702268000 ms.3 from job set of time 1435702268000 ms
-------------------------------------------
Time: 1435702268000 ms
-------------------------------------------

15/06/30 23:11:08 INFO JobScheduler: Finished job streaming job 1435702268000 ms.3 from job set of time 1435702268000 ms
15/06/30 23:11:08 INFO JobScheduler: Starting job streaming job 1435702268000 ms.4 from job set of time 1435702268000 ms
15/06/30 23:11:08 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:08 INFO DAGScheduler: Job 6 finished: foreachRDD at FilterControl.scala:19, took 0.000017 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:08 INFO JobScheduler: Finished job streaming job 1435702268000 ms.4 from job set of time 1435702268000 ms
15/06/30 23:11:08 INFO JobScheduler: Total delay: 0.027 s for time 1435702268000 ms (execution: 0.015 s)
15/06/30 23:11:08 INFO BlockRDD: Removing RDD 4 from persistence list
15/06/30 23:11:08 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[4] at createStream at Spark.scala:31 of time 1435702268000 ms
15/06/30 23:11:08 INFO MapPartitionsRDD: Removing RDD 5 from persistence list
15/06/30 23:11:08 INFO BlockManager: Removing RDD 4
15/06/30 23:11:08 INFO MapPartitionsRDD: Removing RDD 6 from persistence list
15/06/30 23:11:08 INFO BlockManager: Removing RDD 5
15/06/30 23:11:08 INFO BlockManager: Removing RDD 6
15/06/30 23:11:08 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/06/30 23:11:08 INFO InputInfoTracker: remove old batch metadata:
15/06/30 23:11:10 INFO JobScheduler: Added jobs for time 1435702270000 ms
15/06/30 23:11:10 INFO JobScheduler: Starting job streaming job 1435702270000 ms.0 from job set of time 1435702270000 ms
15/06/30 23:11:10 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:10 INFO DAGScheduler: Job 7 finished: foreachRDD at Spark.scala:34, took 0.000022 s
15/06/30 23:11:10 INFO JobScheduler: Finished job streaming job 1435702270000 ms.0 from job set of time 1435702270000 ms
15/06/30 23:11:10 INFO JobScheduler: Starting job streaming job 1435702270000 ms.1 from job set of time 1435702270000 ms
-------------------------------------------
Time: 1435702270000 ms
-------------------------------------------

15/06/30 23:11:10 INFO JobScheduler: Finished job streaming job 1435702270000 ms.1 from job set of time 1435702270000 ms
15/06/30 23:11:10 INFO JobScheduler: Starting job streaming job 1435702270000 ms.2 from job set of time 1435702270000 ms
15/06/30 23:11:10 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:10 INFO DAGScheduler: Job 8 finished: foreachRDD at Spark.scala:55, took 0.000020 s
15/06/30 23:11:10 INFO JobScheduler: Finished job streaming job 1435702270000 ms.2 from job set of time 1435702270000 ms
15/06/30 23:11:10 INFO JobScheduler: Starting job streaming job 1435702270000 ms.3 from job set of time 1435702270000 ms
-------------------------------------------
Time: 1435702270000 ms
-------------------------------------------

15/06/30 23:11:10 INFO JobScheduler: Finished job streaming job 1435702270000 ms.3 from job set of time 1435702270000 ms
15/06/30 23:11:10 INFO JobScheduler: Starting job streaming job 1435702270000 ms.4 from job set of time 1435702270000 ms
15/06/30 23:11:10 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:10 INFO DAGScheduler: Job 9 finished: foreachRDD at FilterControl.scala:19, took 0.000015 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:10 INFO JobScheduler: Finished job streaming job 1435702270000 ms.4 from job set of time 1435702270000 ms
15/06/30 23:11:10 INFO BlockRDD: Removing RDD 7 from persistence list
15/06/30 23:11:10 INFO JobScheduler: Total delay: 0.026 s for time 1435702270000 ms (execution: 0.015 s)
15/06/30 23:11:10 INFO BlockManager: Removing RDD 7
15/06/30 23:11:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[7] at createStream at Spark.scala:31 of time 1435702270000 ms
15/06/30 23:11:10 INFO MapPartitionsRDD: Removing RDD 8 from persistence list
15/06/30 23:11:10 INFO BlockManager: Removing RDD 8
15/06/30 23:11:10 INFO MapPartitionsRDD: Removing RDD 9 from persistence list
15/06/30 23:11:10 INFO BlockManager: Removing RDD 9
15/06/30 23:11:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1435702266000 ms)
15/06/30 23:11:10 INFO InputInfoTracker: remove old batch metadata: 1435702266000 ms
15/06/30 23:11:12 INFO JobScheduler: Added jobs for time 1435702272000 ms
15/06/30 23:11:12 INFO JobScheduler: Starting job streaming job 1435702272000 ms.0 from job set of time 1435702272000 ms
15/06/30 23:11:12 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:12 INFO DAGScheduler: Job 10 finished: foreachRDD at Spark.scala:34, took 0.000017 s
15/06/30 23:11:12 INFO JobScheduler: Finished job streaming job 1435702272000 ms.0 from job set of time 1435702272000 ms
15/06/30 23:11:12 INFO JobScheduler: Starting job streaming job 1435702272000 ms.1 from job set of time 1435702272000 ms
-------------------------------------------
Time: 1435702272000 ms
-------------------------------------------

15/06/30 23:11:12 INFO JobScheduler: Finished job streaming job 1435702272000 ms.1 from job set of time 1435702272000 ms
15/06/30 23:11:12 INFO JobScheduler: Starting job streaming job 1435702272000 ms.2 from job set of time 1435702272000 ms
15/06/30 23:11:12 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:12 INFO DAGScheduler: Job 11 finished: foreachRDD at Spark.scala:55, took 0.000014 s
15/06/30 23:11:12 INFO JobScheduler: Finished job streaming job 1435702272000 ms.2 from job set of time 1435702272000 ms
15/06/30 23:11:12 INFO JobScheduler: Starting job streaming job 1435702272000 ms.3 from job set of time 1435702272000 ms
-------------------------------------------
Time: 1435702272000 ms
-------------------------------------------

15/06/30 23:11:12 INFO JobScheduler: Finished job streaming job 1435702272000 ms.3 from job set of time 1435702272000 ms
15/06/30 23:11:12 INFO JobScheduler: Starting job streaming job 1435702272000 ms.4 from job set of time 1435702272000 ms
15/06/30 23:11:12 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:12 INFO DAGScheduler: Job 12 finished: foreachRDD at FilterControl.scala:19, took 0.000022 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:12 INFO JobScheduler: Finished job streaming job 1435702272000 ms.4 from job set of time 1435702272000 ms
15/06/30 23:11:12 INFO JobScheduler: Total delay: 0.022 s for time 1435702272000 ms (execution: 0.013 s)
15/06/30 23:11:12 INFO BlockRDD: Removing RDD 10 from persistence list
15/06/30 23:11:12 INFO BlockManager: Removing RDD 10
15/06/30 23:11:12 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[10] at createStream at Spark.scala:31 of time 1435702272000 ms
15/06/30 23:11:12 INFO MapPartitionsRDD: Removing RDD 11 from persistence list
15/06/30 23:11:12 INFO BlockManager: Removing RDD 11
15/06/30 23:11:12 INFO MapPartitionsRDD: Removing RDD 12 from persistence list
15/06/30 23:11:12 INFO BlockManager: Removing RDD 12
15/06/30 23:11:12 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1435702268000 ms)
15/06/30 23:11:12 INFO InputInfoTracker: remove old batch metadata: 1435702268000 ms
^C15/06/30 23:11:12 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
15/06/30 23:11:12 INFO ReceiverTracker: Sent stop signal to all 0 receivers
^C^C^C^C15/06/30 23:11:14 INFO JobScheduler: Added jobs for time 1435702274000 ms
15/06/30 23:11:14 INFO JobScheduler: Starting job streaming job 1435702274000 ms.0 from job set of time 1435702274000 ms
15/06/30 23:11:14 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:14 INFO DAGScheduler: Job 13 finished: foreachRDD at Spark.scala:34, took 0.000020 s
15/06/30 23:11:14 INFO JobScheduler: Finished job streaming job 1435702274000 ms.0 from job set of time 1435702274000 ms
15/06/30 23:11:14 INFO JobScheduler: Starting job streaming job 1435702274000 ms.1 from job set of time 1435702274000 ms
-------------------------------------------
Time: 1435702274000 ms
-------------------------------------------

15/06/30 23:11:14 INFO JobScheduler: Finished job streaming job 1435702274000 ms.1 from job set of time 1435702274000 ms
15/06/30 23:11:14 INFO JobScheduler: Starting job streaming job 1435702274000 ms.2 from job set of time 1435702274000 ms
15/06/30 23:11:14 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:14 INFO DAGScheduler: Job 14 finished: foreachRDD at Spark.scala:55, took 0.000012 s
15/06/30 23:11:14 INFO JobScheduler: Finished job streaming job 1435702274000 ms.2 from job set of time 1435702274000 ms
15/06/30 23:11:14 INFO JobScheduler: Starting job streaming job 1435702274000 ms.3 from job set of time 1435702274000 ms
-------------------------------------------
Time: 1435702274000 ms
-------------------------------------------

15/06/30 23:11:14 INFO JobScheduler: Finished job streaming job 1435702274000 ms.3 from job set of time 1435702274000 ms
15/06/30 23:11:14 INFO JobScheduler: Starting job streaming job 1435702274000 ms.4 from job set of time 1435702274000 ms
15/06/30 23:11:14 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:14 INFO DAGScheduler: Job 15 finished: foreachRDD at FilterControl.scala:19, took 0.000021 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:14 INFO JobScheduler: Finished job streaming job 1435702274000 ms.4 from job set of time 1435702274000 ms
15/06/30 23:11:14 INFO BlockRDD: Removing RDD 13 from persistence list
15/06/30 23:11:14 INFO JobScheduler: Total delay: 0.022 s for time 1435702274000 ms (execution: 0.014 s)
15/06/30 23:11:14 INFO BlockManager: Removing RDD 13
15/06/30 23:11:14 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[13] at createStream at Spark.scala:31 of time 1435702274000 ms
15/06/30 23:11:14 INFO MapPartitionsRDD: Removing RDD 14 from persistence list
15/06/30 23:11:14 INFO MapPartitionsRDD: Removing RDD 15 from persistence list
15/06/30 23:11:14 INFO BlockManager: Removing RDD 14
15/06/30 23:11:14 INFO BlockManager: Removing RDD 15
15/06/30 23:11:14 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1435702270000 ms)
15/06/30 23:11:14 INFO InputInfoTracker: remove old batch metadata: 1435702270000 ms
^C^C^C^C^C15/06/30 23:11:16 INFO JobScheduler: Added jobs for time 1435702276000 ms
15/06/30 23:11:16 INFO JobScheduler: Starting job streaming job 1435702276000 ms.0 from job set of time 1435702276000 ms
15/06/30 23:11:16 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:16 INFO DAGScheduler: Job 16 finished: foreachRDD at Spark.scala:34, took 0.000020 s
15/06/30 23:11:16 INFO JobScheduler: Finished job streaming job 1435702276000 ms.0 from job set of time 1435702276000 ms
15/06/30 23:11:16 INFO JobScheduler: Starting job streaming job 1435702276000 ms.1 from job set of time 1435702276000 ms
-------------------------------------------
Time: 1435702276000 ms
-------------------------------------------

15/06/30 23:11:16 INFO JobScheduler: Finished job streaming job 1435702276000 ms.1 from job set of time 1435702276000 ms
15/06/30 23:11:16 INFO JobScheduler: Starting job streaming job 1435702276000 ms.2 from job set of time 1435702276000 ms
15/06/30 23:11:16 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:16 INFO DAGScheduler: Job 17 finished: foreachRDD at Spark.scala:55, took 0.000022 s
15/06/30 23:11:16 INFO JobScheduler: Finished job streaming job 1435702276000 ms.2 from job set of time 1435702276000 ms
15/06/30 23:11:16 INFO JobScheduler: Starting job streaming job 1435702276000 ms.3 from job set of time 1435702276000 ms
-------------------------------------------
Time: 1435702276000 ms
-------------------------------------------

15/06/30 23:11:16 INFO JobScheduler: Finished job streaming job 1435702276000 ms.3 from job set of time 1435702276000 ms
15/06/30 23:11:16 INFO JobScheduler: Starting job streaming job 1435702276000 ms.4 from job set of time 1435702276000 ms
15/06/30 23:11:16 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:16 INFO DAGScheduler: Job 18 finished: foreachRDD at FilterControl.scala:19, took 0.000019 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:16 INFO JobScheduler: Finished job streaming job 1435702276000 ms.4 from job set of time 1435702276000 ms
15/06/30 23:11:16 INFO JobScheduler: Total delay: 0.024 s for time 1435702276000 ms (execution: 0.015 s)
15/06/30 23:11:16 INFO BlockRDD: Removing RDD 16 from persistence list
15/06/30 23:11:16 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[16] at createStream at Spark.scala:31 of time 1435702276000 ms
15/06/30 23:11:16 INFO BlockManager: Removing RDD 16
15/06/30 23:11:16 INFO MapPartitionsRDD: Removing RDD 17 from persistence list
15/06/30 23:11:16 INFO MapPartitionsRDD: Removing RDD 18 from persistence list
15/06/30 23:11:16 INFO BlockManager: Removing RDD 17
15/06/30 23:11:16 INFO BlockManager: Removing RDD 18
15/06/30 23:11:16 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1435702272000 ms)
15/06/30 23:11:16 INFO InputInfoTracker: remove old batch metadata: 1435702272000 ms
^C^C^C^C^C^C15/06/30 23:11:18 INFO JobScheduler: Added jobs for time 1435702278000 ms
15/06/30 23:11:18 INFO JobScheduler: Starting job streaming job 1435702278000 ms.0 from job set of time 1435702278000 ms
15/06/30 23:11:18 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:18 INFO DAGScheduler: Job 19 finished: foreachRDD at Spark.scala:34, took 0.000020 s
15/06/30 23:11:18 INFO JobScheduler: Finished job streaming job 1435702278000 ms.0 from job set of time 1435702278000 ms
15/06/30 23:11:18 INFO JobScheduler: Starting job streaming job 1435702278000 ms.1 from job set of time 1435702278000 ms
-------------------------------------------
Time: 1435702278000 ms
-------------------------------------------

15/06/30 23:11:18 INFO JobScheduler: Finished job streaming job 1435702278000 ms.1 from job set of time 1435702278000 ms
15/06/30 23:11:18 INFO JobScheduler: Starting job streaming job 1435702278000 ms.2 from job set of time 1435702278000 ms
15/06/30 23:11:18 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:18 INFO DAGScheduler: Job 20 finished: foreachRDD at Spark.scala:55, took 0.000016 s
15/06/30 23:11:18 INFO JobScheduler: Finished job streaming job 1435702278000 ms.2 from job set of time 1435702278000 ms
15/06/30 23:11:18 INFO JobScheduler: Starting job streaming job 1435702278000 ms.3 from job set of time 1435702278000 ms
-------------------------------------------
Time: 1435702278000 ms
-------------------------------------------

15/06/30 23:11:18 INFO JobScheduler: Finished job streaming job 1435702278000 ms.3 from job set of time 1435702278000 ms
15/06/30 23:11:18 INFO JobScheduler: Starting job streaming job 1435702278000 ms.4 from job set of time 1435702278000 ms
15/06/30 23:11:18 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:18 INFO DAGScheduler: Job 21 finished: foreachRDD at FilterControl.scala:19, took 0.000019 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:18 INFO JobScheduler: Finished job streaming job 1435702278000 ms.4 from job set of time 1435702278000 ms
15/06/30 23:11:18 INFO BlockRDD: Removing RDD 19 from persistence list
15/06/30 23:11:18 INFO JobScheduler: Total delay: 0.024 s for time 1435702278000 ms (execution: 0.014 s)
15/06/30 23:11:18 INFO BlockManager: Removing RDD 19
15/06/30 23:11:18 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[19] at createStream at Spark.scala:31 of time 1435702278000 ms
15/06/30 23:11:18 INFO MapPartitionsRDD: Removing RDD 20 from persistence list
15/06/30 23:11:18 INFO MapPartitionsRDD: Removing RDD 21 from persistence list
15/06/30 23:11:18 INFO BlockManager: Removing RDD 20
15/06/30 23:11:18 INFO BlockManager: Removing RDD 21
15/06/30 23:11:18 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1435702274000 ms)
15/06/30 23:11:18 INFO InputInfoTracker: remove old batch metadata: 1435702274000 ms
^C^C^C^C^C^C^C^C^C^C^C15/06/30 23:11:19 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
15/06/30 23:11:20 INFO JobScheduler: Added jobs for time 1435702280000 ms
15/06/30 23:11:20 INFO JobScheduler: Starting job streaming job 1435702280000 ms.0 from job set of time 1435702280000 ms
15/06/30 23:11:20 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:20 INFO DAGScheduler: Job 22 finished: foreachRDD at Spark.scala:34, took 0.000020 s
15/06/30 23:11:20 INFO JobScheduler: Finished job streaming job 1435702280000 ms.0 from job set of time 1435702280000 ms
15/06/30 23:11:20 INFO JobScheduler: Starting job streaming job 1435702280000 ms.1 from job set of time 1435702280000 ms
-------------------------------------------
Time: 1435702280000 ms
-------------------------------------------

15/06/30 23:11:20 INFO JobScheduler: Finished job streaming job 1435702280000 ms.1 from job set of time 1435702280000 ms
15/06/30 23:11:20 INFO JobScheduler: Starting job streaming job 1435702280000 ms.2 from job set of time 1435702280000 ms
15/06/30 23:11:20 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:20 INFO DAGScheduler: Job 23 finished: foreachRDD at Spark.scala:55, took 0.000013 s
15/06/30 23:11:20 INFO JobScheduler: Finished job streaming job 1435702280000 ms.2 from job set of time 1435702280000 ms
15/06/30 23:11:20 INFO JobScheduler: Starting job streaming job 1435702280000 ms.3 from job set of time 1435702280000 ms
-------------------------------------------
Time: 1435702280000 ms
-------------------------------------------

15/06/30 23:11:20 INFO JobScheduler: Finished job streaming job 1435702280000 ms.3 from job set of time 1435702280000 ms
15/06/30 23:11:20 INFO JobScheduler: Starting job streaming job 1435702280000 ms.4 from job set of time 1435702280000 ms
15/06/30 23:11:20 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:20 INFO DAGScheduler: Job 24 finished: foreachRDD at FilterControl.scala:19, took 0.000015 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:20 INFO JobScheduler: Finished job streaming job 1435702280000 ms.4 from job set of time 1435702280000 ms
15/06/30 23:11:20 INFO BlockRDD: Removing RDD 22 from persistence list
15/06/30 23:11:20 INFO JobScheduler: Total delay: 0.022 s for time 1435702280000 ms (execution: 0.014 s)
15/06/30 23:11:20 INFO BlockManager: Removing RDD 22
15/06/30 23:11:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[22] at createStream at Spark.scala:31 of time 1435702280000 ms
15/06/30 23:11:20 INFO MapPartitionsRDD: Removing RDD 23 from persistence list
15/06/30 23:11:20 INFO MapPartitionsRDD: Removing RDD 24 from persistence list
15/06/30 23:11:20 INFO BlockManager: Removing RDD 23
15/06/30 23:11:20 INFO BlockManager: Removing RDD 24
15/06/30 23:11:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1435702276000 ms)
15/06/30 23:11:20 INFO InputInfoTracker: remove old batch metadata: 1435702276000 ms
^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C15/06/30 23:11:22 INFO JobScheduler: Added jobs for time 1435702282000 ms
15/06/30 23:11:22 INFO JobScheduler: Starting job streaming job 1435702282000 ms.0 from job set of time 1435702282000 ms
15/06/30 23:11:22 INFO SparkContext: Starting job: foreachRDD at Spark.scala:34
15/06/30 23:11:22 INFO DAGScheduler: Job 25 finished: foreachRDD at Spark.scala:34, took 0.000018 s
15/06/30 23:11:22 INFO JobScheduler: Finished job streaming job 1435702282000 ms.0 from job set of time 1435702282000 ms
15/06/30 23:11:22 INFO JobScheduler: Starting job streaming job 1435702282000 ms.1 from job set of time 1435702282000 ms
-------------------------------------------
Time: 1435702282000 ms
-------------------------------------------

15/06/30 23:11:22 INFO JobScheduler: Finished job streaming job 1435702282000 ms.1 from job set of time 1435702282000 ms
15/06/30 23:11:22 INFO JobScheduler: Starting job streaming job 1435702282000 ms.2 from job set of time 1435702282000 ms
15/06/30 23:11:22 INFO SparkContext: Starting job: foreachRDD at Spark.scala:55
15/06/30 23:11:22 INFO DAGScheduler: Job 26 finished: foreachRDD at Spark.scala:55, took 0.000020 s
15/06/30 23:11:22 INFO JobScheduler: Finished job streaming job 1435702282000 ms.2 from job set of time 1435702282000 ms
15/06/30 23:11:22 INFO JobScheduler: Starting job streaming job 1435702282000 ms.3 from job set of time 1435702282000 ms
-------------------------------------------
Time: 1435702282000 ms
-------------------------------------------

15/06/30 23:11:22 INFO JobScheduler: Finished job streaming job 1435702282000 ms.3 from job set of time 1435702282000 ms
15/06/30 23:11:22 INFO JobScheduler: Starting job streaming job 1435702282000 ms.4 from job set of time 1435702282000 ms
15/06/30 23:11:22 INFO SparkContext: Starting job: foreachRDD at FilterControl.scala:19
15/06/30 23:11:22 INFO DAGScheduler: Job 27 finished: foreachRDD at FilterControl.scala:19, took 0.000021 s
A GUARDAR AS HASHTAGS NO REDIS
--------------------- Hash_tam_redis: 0
15/06/30 23:11:22 INFO JobScheduler: Finished job streaming job 1435702282000 ms.4 from job set of time 1435702282000 ms
15/06/30 23:11:22 INFO JobScheduler: Total delay: 0.021 s for time 1435702282000 ms (execution: 0.013 s)
15/06/30 23:11:22 INFO BlockRDD: Removing RDD 25 from persistence list
15/06/30 23:11:22 INFO BlockManager: Removing RDD 25
15/06/30 23:11:22 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[25] at createStream at Spark.scala:31 of time 1435702282000 ms
15/06/30 23:11:22 INFO MapPartitionsRDD: Removing RDD 26 from persistence list
15/06/30 23:11:22 INFO MapPartitionsRDD: Removing RDD 27 from persistence list
15/06/30 23:11:22 INFO BlockManager: Removing RDD 26
15/06/30 23:11:22 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1435702278000 ms)
15/06/30 23:11:22 INFO BlockManager: Removing RDD 27
15/06/30 23:11:22 INFO InputInfoTracker: remove old batch metadata: 1435702278000 ms
15/06/30 23:11:22 INFO ReceiverTracker: All of the receivers have deregistered successfully
15/06/30 23:11:22 INFO ReceiverTracker: ReceiverTracker stopped
15/06/30 23:11:22 INFO JobGenerator: Stopping JobGenerator immediately
15/06/30 23:11:22 INFO RecurringTimer: Stopped timer for JobGenerator after time 1435702282000
15/06/30 23:11:22 INFO JobGenerator: Stopped JobGenerator
15/06/30 23:11:22 INFO JobScheduler: Stopped JobScheduler
15/06/30 23:11:22 INFO StreamingContext: StreamingContext stopped successfully
15/06/30 23:11:22 INFO SparkContext: Invoking stop() from shutdown hook
^C15/06/30 23:11:22 INFO SparkUI: Stopped Spark web UI at http://192.168.1.101:4040
15/06/30 23:11:22 INFO DAGScheduler: Stopping DAGScheduler
15/06/30 23:11:22 INFO DAGScheduler: Job 0 failed: start at Spark.scala:40, took 17.783752 s
Exception in thread "Thread-27" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:736)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:735)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:735)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1468)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1403)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1642)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:559)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2292)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2262)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2262)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2262)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2244)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
15/06/30 23:11:22 INFO DAGScheduler: ShuffleMapStage 0 (start at Spark.scala:40) failed in 17.620 s
15/06/30 23:11:22 INFO SparkDeploySchedulerBackend: Shutting down all executors
15/06/30 23:11:22 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
15/06/30 23:11:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/06/30 23:11:22 INFO Utils: path = /private/var/folders/cq/rjz7hsgn2w3bjqvvpxglsvfh0000gn/T/spark-72b09a51-f959-4312-a811-59cf88e50f6b/blockmgr-fa5271d6-2b34-4072-8835-d39cf039d0dc, already present as root for deletion.
15/06/30 23:11:22 INFO MemoryStore: MemoryStore cleared
15/06/30 23:11:22 INFO BlockManager: BlockManager stopped
15/06/30 23:11:22 INFO BlockManagerMaster: BlockManagerMaster stopped
15/06/30 23:11:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/06/30 23:11:22 INFO SparkContext: Successfully stopped SparkContext
15/06/30 23:11:22 INFO Utils: Shutdown hook called
15/06/30 23:11:22 INFO Utils: Deleting directory /private/var/folders/cq/rjz7hsgn2w3bjqvvpxglsvfh0000gn/T/spark-72b09a51-f959-4312-a811-59cf88e50f6b